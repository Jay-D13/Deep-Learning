{"cells":[{"cell_type":"markdown","metadata":{"id":"uTv0D26B9W2h"},"source":["# Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"O9VX-OHxC1FM"},"source":["## Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qFHMMDtSwuW4"},"outputs":[],"source":["#@title Mount your Google Drive\n","# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n","# you can delete this cell which is specific to Google Colab. You may also\n","# change the paths for data/logs in Arguments below.\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"oODLwt1QzgGa"},"outputs":[],"source":["#@title Link your assignment folder & install requirements\n","#@markdown Enter the path to the assignment folder in your Google Drive\n","# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n","# you can delete this cell which is specific to Google Colab. You may also\n","# change the paths for data/logs in Arguments below.\n","import sys\n","import os\n","import shutil\n","import warnings\n","\n","folder = \"\" #@param {type:\"string\"}\n","!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n","\n","# Add the assignment folder to Python path\n","if '/content/assignment' not in sys.path:\n","  sys.path.insert(0, '/content/assignment')\n","\n","# Install requirements\n","!pip install -qr /content/assignment/requirements.txt\n","\n","# Check if CUDA is available\n","import torch\n","if not torch.cuda.is_available():\n","  warnings.warn('CUDA is not available.')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dt3NTvpsy4Oc"},"source":["### Running on GPU\n","For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n","* (EN) `Edit > Notebook Settings`\n","* (FR) `Modifier > Param√®tres du notebook`\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"RLVSmv9HoMH5"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import urllib.request\n","\n","from dataclasses import dataclass\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","from lstm_solution import LSTM\n","from gpt1_solution import MiniGPT1\n","from utils.wikitext2 import Wikitext2\n","from utils.torch_utils import seed_experiment, to_device\n","from utils.gpu_usage import get_gpu_memory_usage\n","from utils.data_utils import save_logs\n","from run_exp import train, evaluate\n","\n","EMBEDDINGS_URL = \"https://www.dropbox.com/s/g91502hubcmb4ob/embeddings.npz?dl=0\""]},{"cell_type":"markdown","metadata":{"id":"MZr3Fh-qaGAZ"},"source":["## Public tests\n","Run the following cell in order to run the public tests to check to tensor shapes of the outputs of your functions."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GRwCZpSaaE9V"},"outputs":[{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/__main__.py\", line 18, in <module>\n","    main(module=None)\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/main.py\", line 101, in __init__\n","    self.parseArgs(argv)\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/main.py\", line 127, in parseArgs\n","    self._do_discovery(argv[2:])\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/main.py\", line 247, in _do_discovery\n","    self.createTests(from_discovery=True, Loader=Loader)\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/main.py\", line 157, in createTests\n","    self.test = loader.discover(self.start, self.pattern, self.top)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/jaydan/anaconda3/envs/ift6135/lib/python3.11/unittest/loader.py\", line 320, in discover\n","    raise ImportError('Start directory is not importable: %r' % start_dir)\n","ImportError: Start directory is not importable: '/content/assignment/'\n"]}],"source":["!python -m unittest discover -s /content/assignment/"]},{"cell_type":"markdown","metadata":{"id":"-PtvL_yKp3PW"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"iWiJme7XaLiR"},"source":["Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularization, adding logs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUrqebfCobD1"},"outputs":[],"source":["@dataclass\n","class Arguments:\n","  # Data\n","  data_folder: str = '/content/assignment/data'\n","  batch_size: int = 16\n","\n","  # Model\n","  model: str = 'lstm'  # [lstm, gpt1]\n","  embeddings: str = '/content/assignment/data/embeddings.npz'\n","  layers: int = 1\n","\n","  # Optimization\n","  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n","  epochs: int = 10\n","  lr: float = 1e-3\n","  momentum: float = 0.9\n","  weight_decay: float = 5e-4\n","\n","  # Experiment\n","  exp_id: str = 'debug'\n","  log: bool = True\n","  log_dir: str = '/content/assignment/logs'\n","  seed: int = 42\n","\n","  # Miscellaneous\n","  num_workers: int = 2\n","  device: str = 'cuda'\n","  progress_bar: bool = False\n","  print_every: int = 10"]},{"cell_type":"markdown","metadata":{"id":"5ntfY6yyad_F"},"source":["The 12 configurations you need to run in Problem 3. Be careful that there is no discrepency between the configurations defined in `run_exp.py` and the ones below. In case there is a difference, the version from `run_exp.py` should be considered the ones to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q6AwUVDX78-"},"outputs":[],"source":["# Note: if there is any discrepency with the configurations in run_exp.py, the\n","# version from run_exp.py should be the ones to use in Problem 3.\n","configs = {\n","  1: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id='1'),\n","  2: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='2'),\n","  3: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id='3'),\n","  4: Arguments(model='lstm', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id='4'),\n","\n","  5: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adam', exp_id='5'),\n","  6: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='6'),\n","  7: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='sgd', exp_id='7'),\n","  8: Arguments(model='gpt1', layers=1, batch_size=16, log=True, epochs=10, optimizer='momentum', exp_id='8'),\n","\n","  9: Arguments(model='lstm', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='9'),\n","  10: Arguments(model='lstm', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='10'),\n","  11: Arguments(model='gpt1', layers=2, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='11'),\n","  12: Arguments(model='gpt1', layers=4, batch_size=16, log=True, epochs=10, optimizer='adamw', exp_id='12'),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2rjoY-5phTY"},"outputs":[],"source":["def main(args):\n","  \n","  torch.cuda.empty_cache()\n","  \n","  # Seed the experiment, for repeatability\n","  seed_experiment(args.seed)\n","\n","  # Dataloaders\n","  train_dataset = Wikitext2(args.data_folder, split=\"train\")\n","  train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=args.batch_size,\n","    shuffle=True,\n","    num_workers=args.num_workers,\n","  )\n","\n","  valid_dataset = Wikitext2(args.data_folder, split=\"validation\")\n","  valid_dataloader = DataLoader(\n","    valid_dataset,\n","    batch_size=args.batch_size,\n","    shuffle=False,\n","    num_workers=args.num_workers,\n","  )\n","\n","  test_dataset = Wikitext2(args.data_folder, split=\"test\")\n","  test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=args.batch_size,\n","    shuffle=False,\n","    num_workers=args.num_workers,\n","  )\n","\n","  # Download the embeddings\n","  if not os.path.isfile(args.embeddings):\n","    print(\"Downloading embeddings...\")\n","    urllib.request.urlretrieve(EMBEDDINGS_URL, args.embeddings)\n","\n","  # Model\n","  if args.model == \"lstm\":\n","    model = LSTM.load_embeddings_from(\n","      args.embeddings, hidden_size=512, num_layers=args.layers\n","    )\n","  elif args.model == \"gpt1\":\n","    model = MiniGPT1.load_embeddings_from(\n","      args.embeddings, num_layers=args.layers\n","    )\n","  else:\n","    raise ValueError(\"Unknown model {0}\".format(args.model))\n","  model.to(args.device)\n","\n","  # Optimizer\n","  if args.optimizer == \"adamw\":\n","    optimizer = optim.AdamW(\n","      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n","    )\n","  elif args.optimizer == \"adam\":\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","  elif args.optimizer == \"sgd\":\n","    optimizer = optim.SGD(\n","      model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n","    )\n","  elif args.optimizer == \"momentum\":\n","    optimizer = optim.SGD(\n","      model.parameters(),\n","      lr=args.lr,\n","      momentum=args.momentum,\n","      weight_decay=args.weight_decay,\n","    )\n","\n","  print(\n","    f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n","    f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n","  )\n","\n","  train_losses, valid_losses = [], []\n","  train_ppls, valid_ppls = [], []\n","  train_times, valid_times = [], []\n","  gpu_memories = []\n","  for epoch in range(args.epochs):\n","\n","    tqdm.write(f\"====== Epoch {epoch} ======>\")\n","    \n","    mem_before = get_gpu_memory_usage()\n","\n","    loss, ppl, wall_time = train(epoch, model, train_dataloader, optimizer, args)\n","    train_losses.append(loss)\n","    train_ppls.append(ppl)\n","    train_times.append(wall_time)\n","\n","    loss, ppl, wall_time = evaluate(epoch, model, valid_dataloader, args)\n","    valid_losses.append(loss)\n","    valid_ppls.append(ppl)\n","    valid_times.append(wall_time)\n","    \n","    mem_after = get_gpu_memory_usage()\n","    avg_memory = (mem_before + mem_after) / 2\n","    gpu_memories.append(avg_memory)\n","\n","  test_loss, test_ppl, test_time = evaluate(\n","    epoch, model, test_dataloader, args, mode=\"test\"\n","  )\n","\n","  avg_steady_state_memory = sum(gpu_memories) / len(gpu_memories)\n","  print(f\"Average steady-state GPU memory usage: {avg_steady_state_memory:.2f} MB\")  \n","  \n","  print(f\"===== Best validation perplexity: {min(valid_ppls):.3f} =====>\")\n","\n","  return (\n","    train_losses,\n","    train_ppls,\n","    train_times,\n","    valid_losses,\n","    valid_ppls,\n","    valid_times,\n","    test_loss,\n","    test_ppl,\n","    test_time,\n","    avg_steady_state_memory\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyJPWO1ppcTx"},"outputs":[],"source":["args = configs[1]  # Run the first configuration\n","logs = main(args)\n","if args.log:\n","  save_logs(args, *logs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwWZq6zk0YDF"},"outputs":[],"source":["for i in range(1, 13):\n","  args = configs[i]\n","  logs = main(args)\n","  if args.log:\n","    save_logs(args, *logs)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import json\n","import os\n","import matplotlib.pyplot as plt\n","\n","def load_json(filepath):\n","    with open(filepath, 'r') as f:\n","        return json.load(f)\n","\n","def load_txt(filepath):\n","    with open(filepath, 'r') as f:\n","        return [float(line.strip()) for line in f]\n","    \n","def save_plot(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    return path"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import os\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","base_dir = \"logs/\"\n","save_folder = \"plots/\"\n","\n","def plot_for_experiment(exp_id):\n","    \n","    # Load data\n","    args_path = os.path.join(base_dir, str(exp_id), \"args.json\")\n","    with open(args_path, 'r') as f:\n","        args = json.load(f)\n","\n","    train_loss = np.loadtxt(os.path.join(base_dir, str(exp_id), \"train_loss.txt\"))\n","    valid_loss = np.loadtxt(os.path.join(base_dir, str(exp_id), \"valid_loss.txt\"))\n","    train_ppl = np.loadtxt(os.path.join(base_dir, str(exp_id), \"train_ppl.txt\"))\n","    valid_ppl = np.loadtxt(os.path.join(base_dir, str(exp_id), \"valid_ppl.txt\"))\n","    train_time = np.cumsum(np.loadtxt(os.path.join(base_dir, str(exp_id), \"train_time.txt\")))\n","    valid_time = np.cumsum(np.loadtxt(os.path.join(base_dir, str(exp_id), \"valid_time.txt\")))\n","\n","    caption = (\n","        f\"Model: {args['model']}, Layers: {args['layers']}, Optimizer: {args['optimizer']}, \"\n","        f\"Learning Rate: {args['lr']}, Momentum: {args['momentum']}, Weight Decay: {args['weight_decay']}, \"\n","        f\"Batch Size: {args['batch_size']}\"\n","    )\n","\n","    # Plots\n","    fig, axs = plt.subplots(3, 2, figsize=(15, 20))\n","    fig.suptitle(f\"Experiment {exp_id}'s logs in graphs\", fontsize=24)\n","    fig.text(0.5, 0.04, caption, ha='center', fontsize=16)\n","\n","    # Training loss over time\n","    axs[0, 0].plot(train_time, train_loss, label='Train Loss')\n","    axs[0, 0].set_title('Training Loss Over Time')\n","    axs[0, 0].set_xlabel('Time (s)')\n","    axs[0, 0].set_ylabel('Loss')\n","    axs[0, 0].legend()\n","\n","    # Validation loss over time\n","    axs[0, 1].plot(valid_time, valid_loss, label='Validation Loss', color='r')\n","    axs[0, 1].set_title('Validation Loss Over Time')\n","    axs[0, 1].set_xlabel('Time (s)')\n","    axs[0, 1].set_ylabel('Loss')\n","    axs[0, 1].legend()\n","\n","    # Training perplexity over time\n","    axs[1, 0].plot(train_time, train_ppl, label='Train Perplexity')\n","    axs[1, 0].set_title('Training Perplexity Over Time')\n","    axs[1, 0].set_xlabel('Time (s)')\n","    axs[1, 0].set_ylabel('Perplexity')\n","    axs[1, 0].legend()\n","\n","    # Validation perplexity over time\n","    axs[1, 1].plot(valid_time, valid_ppl, label='Validation Perplexity', color='r')\n","    axs[1, 1].set_title('Validation Perplexity Over Time')\n","    axs[1, 1].set_xlabel('Time (s)')\n","    axs[1, 1].set_ylabel('Perplexity')\n","    axs[1, 1].legend()\n","\n","    # Training and Validation loss over epoch\n","    epochs = list(range(1, len(train_loss)+1))\n","    axs[2, 0].plot(epochs, train_loss, label='Train Loss')\n","    axs[2, 0].plot(epochs, valid_loss, label='Validation Loss', linestyle='--')\n","    axs[2, 0].set_title('Training and Validation Loss Over Epoch')\n","    axs[2, 0].set_xlabel('Epoch')\n","    axs[2, 0].set_ylabel('Loss')\n","    axs[2, 0].legend()\n","\n","    # Training and Validation perplexity over epoch\n","    axs[2, 1].plot(epochs, train_ppl, label='Train Perplexity')\n","    axs[2, 1].plot(epochs, valid_ppl, label='Validation Perplexity', linestyle='--')\n","    axs[2, 1].set_title('Training and Validation Perplexity Over Epoch')\n","    axs[2, 1].set_xlabel('Epoch')\n","    axs[2, 1].set_ylabel('Perplexity')\n","    axs[2, 1].legend()\n","\n","    # save_path = os.path.join(c, str(exp_id))\n","    os.makedirs(save_folder, exist_ok=True)\n","    fig.savefig(os.path.join(save_folder, f\"experiment_{exp_id}.png\"))\n","    plt.close(fig)\n","\n","for exp_id in range(1, 13):\n","    plot_for_experiment(exp_id)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Experiment Number': 5, 'Architecture': 'gpt1', 'Number of Layers': 1, 'Optimizer': 'adam', 'Best Validation Perplexity': 1.112, 'Training Perplexity at Best Epoch': 1.09683, 'best_epoch': 3}\n","{'Experiment Number': 6, 'Architecture': 'gpt1', 'Number of Layers': 1, 'Optimizer': 'adamw', 'Best Validation Perplexity': 1.11202, 'Training Perplexity at Best Epoch': 1.09685, 'best_epoch': 3}\n","{'Experiment Number': 8, 'Architecture': 'gpt1', 'Number of Layers': 1, 'Optimizer': 'momentum', 'Best Validation Perplexity': 669.23708, 'Training Perplexity at Best Epoch': 702.54616, 'best_epoch': 9}\n","{'Experiment Number': 7, 'Architecture': 'gpt1', 'Number of Layers': 1, 'Optimizer': 'sgd', 'Best Validation Perplexity': 1422.62489, 'Training Perplexity at Best Epoch': 1528.48684, 'best_epoch': 9}\n","{'Experiment Number': 11, 'Architecture': 'gpt1', 'Number of Layers': 2, 'Optimizer': 'adamw', 'Best Validation Perplexity': 1.35718, 'Training Perplexity at Best Epoch': 1.38032, 'best_epoch': 7}\n","{'Experiment Number': 12, 'Architecture': 'gpt1', 'Number of Layers': 4, 'Optimizer': 'adamw', 'Best Validation Perplexity': 1479.1038, 'Training Perplexity at Best Epoch': 1552.5376, 'best_epoch': 9}\n","{'Experiment Number': 1, 'Architecture': 'lstm', 'Number of Layers': 1, 'Optimizer': 'adam', 'Best Validation Perplexity': 144.22962, 'Training Perplexity at Best Epoch': 81.06126, 'best_epoch': 9}\n","{'Experiment Number': 2, 'Architecture': 'lstm', 'Number of Layers': 1, 'Optimizer': 'adamw', 'Best Validation Perplexity': 145.19584, 'Training Perplexity at Best Epoch': 81.36937, 'best_epoch': 9}\n","{'Experiment Number': 4, 'Architecture': 'lstm', 'Number of Layers': 1, 'Optimizer': 'momentum', 'Best Validation Perplexity': 1654.22523, 'Training Perplexity at Best Epoch': 1748.53919, 'best_epoch': 9}\n","{'Experiment Number': 3, 'Architecture': 'lstm', 'Number of Layers': 1, 'Optimizer': 'sgd', 'Best Validation Perplexity': 2173.37962, 'Training Perplexity at Best Epoch': 2322.20604, 'best_epoch': 9}\n","{'Experiment Number': 9, 'Architecture': 'lstm', 'Number of Layers': 2, 'Optimizer': 'adamw', 'Best Validation Perplexity': 141.33181, 'Training Perplexity at Best Epoch': 82.7597, 'best_epoch': 6}\n","{'Experiment Number': 10, 'Architecture': 'lstm', 'Number of Layers': 4, 'Optimizer': 'adamw', 'Best Validation Perplexity': 159.66512, 'Training Perplexity at Best Epoch': 95.28013, 'best_epoch': 8}\n"]}],"source":["import os\n","import json\n","\n","base_path = \"logs\"\n","\n","results = []\n","\n","for exp_id in list(range(1, 13)) :\n","    exp_path = os.path.join(base_path, str(exp_id))\n","    \n","    # Load args.json to get model configurations\n","    with open(os.path.join(exp_path, \"args.json\"), 'r') as f:\n","        args = json.load(f)\n","\n","    # Load validation perplexities\n","    with open(os.path.join(exp_path, \"valid_ppl.txt\"), 'r') as f:\n","        valid_ppls = [float(line.strip()) for line in f.readlines()]\n","\n","    # Load training perplexities\n","    with open(os.path.join(exp_path, \"train_ppl.txt\"), 'r') as f:\n","        train_ppls = [float(line.strip()) for line in f.readlines()]\n","\n","    best_epoch = valid_ppls.index(min(valid_ppls))\n","\n","    result = {\n","        \"Experiment Number\": exp_id,\n","        \"Architecture\": args[\"model\"],\n","        \"Number of Layers\": args[\"layers\"],\n","        \"Optimizer\": args[\"optimizer\"],\n","        \"Best Validation Perplexity\": round(valid_ppls[best_epoch], 5),\n","        \"Training Perplexity at Best Epoch\": round(train_ppls[best_epoch],5),\n","        \"best_epoch\": best_epoch,\n","    }\n","    results.append(result)\n","\n","# Sort the results based on the given criteria\n","results.sort(key=lambda x: (x[\"Architecture\"], x[\"Number of Layers\"], x[\"Optimizer\"]))\n","\n","for res in results:\n","    print(res)\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fastest configuration based on wall-clock time: {'optimizer': 'sgd', 'architecture': 'gpt1', 'layers': 1, 'total_train_time': 388.8509449958801, 'best_valid_ppl': 1422.624893995365}\n","Best configuration based on generalization performance: {'optimizer': 'adam', 'architecture': 'gpt1', 'layers': 1, 'total_train_time': 392.8220703601837, 'best_valid_ppl': 1.112004132695749}\n"]}],"source":["import os\n","import json\n","\n","def fetch_experiment_data(exp_id):\n","    data = {}\n","    with open(f'logs/{exp_id}/args.json', 'r') as f:\n","        args = json.load(f)\n","        data['optimizer'] = args['optimizer']\n","        data['architecture'] = args['model']\n","        data['layers'] = args['layers']\n","\n","    with open(f'logs/{exp_id}/train_time.txt', 'r') as f:\n","        train_times = [float(line.strip()) for line in f.readlines()]\n","        data['total_train_time'] = sum(train_times)\n","\n","    with open(f'logs/{exp_id}/valid_ppl.txt', 'r') as f:\n","        valid_ppls = [float(line.strip()) for line in f.readlines()]\n","        data['best_valid_ppl'] = min(valid_ppls)\n","\n","    return data\n","\n","experiments = [fetch_experiment_data(exp_id) for exp_id in range(1, 13)]\n","\n","# Sorting by wall-clock time\n","sorted_by_time = sorted(experiments, key=lambda x: x['total_train_time'])\n","fastest_config = sorted_by_time[0]\n","print(f\"Fastest configuration based on wall-clock time: {fastest_config}\")\n","\n","# Sorting by generalization performance\n","sorted_by_perf = sorted(experiments, key=lambda x: x['best_valid_ppl'])\n","best_generalizing_config = sorted_by_perf[0]\n","print(f\"Best configuration based on generalization performance: {best_generalizing_config}\")\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Experiment 1: Total Training Time = 393.91 seconds, Best Validation Perplexity = 144.2296\n","Experiment 2: Total Training Time = 392.3 seconds, Best Validation Perplexity = 145.1958\n","Experiment 3: Total Training Time = 389.61 seconds, Best Validation Perplexity = 2173.3796\n","Experiment 4: Total Training Time = 390.66 seconds, Best Validation Perplexity = 1654.2252\n","Experiment 5: Total Training Time = 392.82 seconds, Best Validation Perplexity = 1.112\n","Experiment 6: Total Training Time = 393.68 seconds, Best Validation Perplexity = 1.112\n","Experiment 7: Total Training Time = 388.85 seconds, Best Validation Perplexity = 1422.6249\n","Experiment 8: Total Training Time = 390.02 seconds, Best Validation Perplexity = 669.2371\n","Experiment 9: Total Training Time = 502.19 seconds, Best Validation Perplexity = 141.3318\n","Experiment 10: Total Training Time = 644.15 seconds, Best Validation Perplexity = 159.6651\n","Experiment 11: Total Training Time = 500.55 seconds, Best Validation Perplexity = 1.3572\n","Experiment 12: Total Training Time = 708.54 seconds, Best Validation Perplexity = 1479.1038\n","\n","\n","Sorted by Total Training Time:\n","Experiment 7: Total Training Time = 388.85 seconds, Best Validation Perplexity = 1422.6249\n","Experiment 3: Total Training Time = 389.61 seconds, Best Validation Perplexity = 2173.3796\n","Experiment 8: Total Training Time = 390.02 seconds, Best Validation Perplexity = 669.2371\n","Experiment 4: Total Training Time = 390.66 seconds, Best Validation Perplexity = 1654.2252\n","Experiment 2: Total Training Time = 392.3 seconds, Best Validation Perplexity = 145.1958\n","Experiment 5: Total Training Time = 392.82 seconds, Best Validation Perplexity = 1.112\n","Experiment 6: Total Training Time = 393.68 seconds, Best Validation Perplexity = 1.112\n","Experiment 1: Total Training Time = 393.91 seconds, Best Validation Perplexity = 144.2296\n","Experiment 11: Total Training Time = 500.55 seconds, Best Validation Perplexity = 1.3572\n","Experiment 9: Total Training Time = 502.19 seconds, Best Validation Perplexity = 141.3318\n","Experiment 10: Total Training Time = 644.15 seconds, Best Validation Perplexity = 159.6651\n","Experiment 12: Total Training Time = 708.54 seconds, Best Validation Perplexity = 1479.1038\n"]}],"source":["def fetch_experiment_data(exp_id):\n","    with open(f'logs/{exp_id}/train_time.txt', 'r') as f:\n","        train_times = [float(line.strip()) for line in f.readlines()]\n","        total_train_time = sum(train_times)\n","\n","    with open(f'logs/{exp_id}/valid_ppl.txt', 'r') as f:\n","        valid_ppls = [float(line.strip()) for line in f.readlines()]\n","        best_valid_ppl = min(valid_ppls)\n","    \n","    return round(total_train_time,2), round(best_valid_ppl,4), exp_id\n","\n","time_and_ppl = [fetch_experiment_data(exp_id) for exp_id in range(1, 13)]\n","for total_train_time, best_valid_ppl, exp_id in time_and_ppl:\n","    print(f\"Experiment {exp_id}: Total Training Time = {total_train_time} seconds, Best Validation Perplexity = {best_valid_ppl}\")\n","    \n","print(\"\\n\\nSorted by Total Training Time:\")\n","sorted_by_time = sorted(time_and_ppl, key=lambda x: x[0])\n","for total_train_time, best_valid_ppl, exp_id in sorted_by_time:\n","    print(f\"Experiment {exp_id}: Total Training Time = {total_train_time} seconds, Best Validation Perplexity = {best_valid_ppl}\")\n","    \n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: adam\n","Average Training Time: 393.36 seconds\n","Average Validation Perplexity: 72.67\n","----------------------------------------\n","Optimizer: adamw\n","Average Training Time: 392.99 seconds\n","Average Validation Perplexity: 73.15\n","----------------------------------------\n","Optimizer: sgd\n","Average Training Time: 389.23 seconds\n","Average Validation Perplexity: 1798.00\n","----------------------------------------\n","Optimizer: momentum\n","Average Training Time: 390.34 seconds\n","Average Validation Perplexity: 1161.73\n","----------------------------------------\n"]}],"source":["import numpy as np\n","\n","experiments = {\n","    1: {\"optimizer\": \"adam\", \"weight_decay\": 0.0, \"momentum\": 0.0, \"train_time\": 393.9062, \"val_ppl\": 144.2296},\n","    2: {\"optimizer\": \"adamw\", \"weight_decay\": 5e-4, \"momentum\": 0.0, \"train_time\": 392.297, \"val_ppl\": 145.1958},\n","    3: {\"optimizer\": \"sgd\", \"weight_decay\": 5e-4, \"momentum\": 0.0, \"train_time\": 389.6111, \"val_ppl\": 2173.3796},\n","    4: {\"optimizer\": \"momentum\", \"weight_decay\": 5e-4, \"momentum\": 0.9, \"train_time\": 390.6608, \"val_ppl\": 1654.2252},\n","    5: {\"optimizer\": \"adam\", \"weight_decay\": 0.0, \"momentum\": 0.0, \"train_time\": 392.8221, \"val_ppl\": 1.112},\n","    6: {\"optimizer\": \"adamw\", \"weight_decay\": 5e-4, \"momentum\": 0.0, \"train_time\": 393.6791, \"val_ppl\": 1.112},\n","    7: {\"optimizer\": \"sgd\", \"weight_decay\": 5e-4, \"momentum\": 0.0, \"train_time\": 388.8509, \"val_ppl\": 1422.6249},\n","    8: {\"optimizer\": \"momentum\", \"weight_decay\": 5e-4, \"momentum\": 0.9, \"train_time\": 390.0189, \"val_ppl\": 669.2371}\n","}\n","\n","# Group by optimizer\n","grouped_results = {}\n","for exp, data in experiments.items():\n","    optimizer = data[\"optimizer\"]\n","    if optimizer not in grouped_results:\n","        grouped_results[optimizer] = {\"train_times\": [], \"val_ppls\": []}\n","    grouped_results[optimizer][\"train_times\"].append(data[\"train_time\"])\n","    grouped_results[optimizer][\"val_ppls\"].append(data[\"val_ppl\"])\n","\n","for optimizer, data in grouped_results.items():\n","    mean_time = np.mean(data[\"train_times\"])\n","    mean_ppl = np.mean(data[\"val_ppls\"])\n","    print(f\"Optimizer: {optimizer}\")\n","    print(f\"Average Training Time: {mean_time:.2f} seconds\")\n","    print(f\"Average Validation Perplexity: {mean_ppl:.2f}\")\n","    print(\"-\" * 40)\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Experiment 1: Average GPU Memory Usage = 4457.7 MB\n","Experiment 2: Average GPU Memory Usage = 4630.0 MB\n","Experiment 3: Average GPU Memory Usage = 4630.0 MB\n","Experiment 4: Average GPU Memory Usage = 4630.0 MB\n","Experiment 5: Average GPU Memory Usage = 4630.0 MB\n","Experiment 6: Average GPU Memory Usage = 4630.0 MB\n","Experiment 7: Average GPU Memory Usage = 4630.0 MB\n","Experiment 8: Average GPU Memory Usage = 4630.0 MB\n","Experiment 9: Average GPU Memory Usage = 4630.0 MB\n","Experiment 10: Average GPU Memory Usage = 4630.0 MB\n","Experiment 11: Average GPU Memory Usage = 5232.3 MB\n","Experiment 12: Average GPU Memory Usage = 5866.3 MB\n"]}],"source":["import os\n","\n","base_dir = \"logs\"\n","\n","gpu_mem_usage = {}\n","for exp_id in range(1, 13):\n","    file_path = os.path.join(base_dir, str(exp_id), \"avg_steady_state_memory.txt\")\n","    \n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            # Read the memory usage, convert to float, and store it\n","            gpu_mem_usage[exp_id] = float(f.read().strip())\n","\n","for exp_id, mem in gpu_mem_usage.items():\n","    print(f\"Experiment {exp_id}: Average GPU Memory Usage = {mem} MB\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"main.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}
